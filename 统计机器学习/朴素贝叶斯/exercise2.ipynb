{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'cute', 'love', 'dog'] 推测为：非侮辱性词汇\n",
      "['stop', 'stupid', 'food'] 推测为：侮辱性词汇\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    " \n",
    "from numpy import *\n",
    "from functools import reduce  \n",
    " \n",
    "#侮辱性词汇标识\n",
    "AbClass = 1\n",
    " \n",
    " \n",
    "def loadDataSet():\n",
    "    \"\"\"加载数据集合及其对应的分类\"\"\"\n",
    "    wordsList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return wordsList, classVec\n",
    " \n",
    " \n",
    "\n",
    "def doc2VecList(docList):\n",
    "    # 从第一个和第二个集合开始进行并集操作，最后返回一个不重复的并集\n",
    "    a = list(reduce(lambda x, y: set(x) | set(y), docList))\n",
    "    #a|b返回结果为两个集合a和b的不重复并集\n",
    "    return a\n",
    " \n",
    "\n",
    "    \n",
    "def words2Vec(vecList, inputWords):\n",
    "    \"\"\"把单子转化为词向量\"\"\"\n",
    "    # 转化成一维数组\n",
    "    resultVec = [0] * len(vecList)\n",
    "    for word in inputWords:\n",
    "        if word in vecList:\n",
    "            # 在单词出现的位置上的计数加1\n",
    "            resultVec[vecList.index(word)] += 1\n",
    "        else:\n",
    "            print('没有发现此单词')\n",
    " \n",
    "    return array(resultVec)\n",
    " \n",
    " \n",
    "def trainNB(trainMatrix, trainClass):\n",
    "    \"\"\"计算，生成每个词对于类别上的概率\"\"\"\n",
    "    \n",
    "    # 类别行数\n",
    "    numTrainClass = len(trainClass)\n",
    "    # 列数\n",
    "    numWords = len(trainMatrix[0])\n",
    " \n",
    "    # 全部都初始化为1， 防止出现概率为0的情况出现\n",
    "    # 见于韩家炜的数据挖掘概念与技术上的讲解，避免出现概率为0的状况，影响计算，因为在数量很大的情况下，在分子和分母同时+1的情况不会\n",
    "    # 影响主要的数据\n",
    "    p0Num = ones(numWords)\n",
    "    p1Num = ones(numWords)\n",
    "    \n",
    "    # 相应的单词初始化为2\n",
    "    # 为了分子分母同时都加上某个数λ\n",
    "    p0Words = 2.0\n",
    "    p1Words = 2.0\n",
    "    \n",
    "    # 统计每个分类的词的总数\n",
    "    # 训练数据集的行数作为遍历的条件，从1开始\n",
    "    # 如果当前类别为1，那么p1Num会加上当前单词矩阵行数据，依次遍历\n",
    "    # 如果当前类别为0，那么p0Num会加上当前单词矩阵行数据，依次遍历\n",
    "    # 同时统计当前类别下单词的个数和p1Words和p0Words\n",
    "    for i in range(numTrainClass):\n",
    "        if trainClass[i] == 1:\n",
    "            # 数组在对应的位置上相加\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Words += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Words += sum(trainMatrix[i])\n",
    "    \n",
    "    # 计算每种类型里面， 每个单词出现的概率\n",
    "    p0Vec = log(p0Num / p0Words)\n",
    "    p1Vec = log(p1Num / p1Words)\n",
    "    \n",
    "    # 计算在类别中1出现的概率，0类出现的概率可通过1-p得到\n",
    "    pClass1 = sum(trainClass) / float(numTrainClass)\n",
    "    \n",
    "    return p0Vec, p1Vec, pClass1\n",
    " \n",
    " \n",
    "#朴素贝叶斯分类函数, max(p0， p1)作为推断的分类\n",
    "def classifyNB(testVec, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(testVec * p1Vec) + log(pClass1)      #此处计算出的p1是用对数表示，因为对数也是单调递增的\n",
    "    p0 = sum(testVec * p0Vec) + log(1 - pClass1)\n",
    "    if p0 > p1:\n",
    "        return 0\n",
    "    return 1\n",
    " \n",
    " \n",
    "def printClass(words, testClass):\n",
    "    if testClass == AbClass:\n",
    "        print(words, '推测为：侮辱性词汇')\n",
    "    else:\n",
    "        print(words, '推测为：非侮辱性词汇')\n",
    " \n",
    "\n",
    "#测试函数\n",
    "def tNB():\n",
    "    # 从训练数据集中提取出属性矩阵和分类数据\n",
    "    docList, classVec = loadDataSet()\n",
    "    \n",
    "    # 生成包含所有单词的list\n",
    "    # 此处生成的单词向量是不重复的\n",
    "    allWordsVec = doc2VecList(docList)\n",
    "    \n",
    "    # 构建词向量矩阵\n",
    "    # 计算docList数据集中每一行每个单词出现的次数，其中返回的trainMat是一个数组的数组\n",
    "    trainMat = list(map(lambda x: words2Vec(allWordsVec, x), docList))\n",
    "    \n",
    "    # 训练计算每个词在分类上的概率, p0V:每个单词在非分类出现的概率， p1V:每个单词在是分类出现的概率\n",
    "    # 其中概率是以ln进行计算的\n",
    "    # pClass1为类别中是1的概率\n",
    "    p0V, p1V, pClass1 = trainNB(trainMat, classVec)\n",
    "    \n",
    "    # 测试数据集\n",
    "    testWords = ['my', 'cute', 'love', 'dog']\n",
    "    \n",
    "    # 转换成单词向量，32个单词构成的数组，如果此单词在数组中，数组的项值置1\n",
    "    testVec = words2Vec(allWordsVec, testWords)\n",
    "    \n",
    "    # 通过将单词向量testVec代入，根据贝叶斯公式，比较各个类别的后验概率，判断当前数据的分类情况\n",
    "    testClass = classifyNB(testVec, p0V, p1V, pClass1)\n",
    "    \n",
    "    # 打印出测试结果\n",
    "    printClass(testWords, testClass)\n",
    "    \n",
    "    #测试数据集\n",
    "    testWords = ['stop', 'stupid', 'food']\n",
    "    \n",
    "    # 转换成单词向量，32个单词构成的数组，如果此单词在数组中，数组的项值置1\n",
    "    testVec = words2Vec(allWordsVec, testWords)\n",
    "    \n",
    "    # 通过将单词向量testVec代入，根据贝叶斯公式，比较各个类别的后验概率，判断当前数据的分类情况\n",
    "    testClass = classifyNB(testVec, p0V, p1V, pClass1)\n",
    "    \n",
    "    # 打印出测试结果\n",
    "    printClass(testWords, testClass)\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    tNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit (conda)",
   "language": "python",
   "name": "python36564bitcondabd881634ef594d9bb40eec9df73ed667"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
