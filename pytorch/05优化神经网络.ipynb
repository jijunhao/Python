{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5288824b",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam([var1, var2], lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cc23f",
   "metadata": {},
   "source": [
    "## 基类\n",
    "\n",
    "| [`Optimizer.add_param_group`](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.add_param_group.html#torch.optim.Optimizer.add_param_group) | 向[`Optimizer`](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer)s param_groups添加一个参数组。 |\n",
    "| ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| [`Optimizer.load_state_dict`](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.load_state_dict.html#torch.optim.Optimizer.load_state_dict) | 加载优化器状态。                                             |\n",
    "| [`Optimizer.state_dict`](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict) | 将优化器的状态作为[`dict`](https://docs.python.org/3/library/stdtypes.html#dict). |\n",
    "| [`Optimizer.step`](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step) | 执行单个优化步骤（参数更新）。                               |\n",
    "| [`Optimizer.zero_grad`](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad) | 将所有优化的 s 的梯度设置[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)为零。 |\n",
    "\n",
    "## 算法\n",
    "\n",
    "| [`Adadelta`](https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html#torch.optim.Adadelta) | 实现 Adadelta 算法。                                         |\n",
    "| ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| [`Adagrad`](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html#torch.optim.Adagrad) | 实现 Adagrad 算法。                                          |\n",
    "| [`Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam) | 实现亚当算法。                                               |\n",
    "| [`AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW) | 实现 AdamW 算法。                                            |\n",
    "| [`SparseAdam`](https://pytorch.org/docs/stable/generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam) | 实现适用于稀疏张量的 Adam 算法的惰性版本。                   |\n",
    "| [`Adamax`](https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html#torch.optim.Adamax) | 实现 Adamax 算法（基于无穷范数的 Adam 变体）。               |\n",
    "| [`ASGD`](https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html#torch.optim.ASGD) | 实现平均随机梯度下降。                                       |\n",
    "| [`LBFGS`](https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html#torch.optim.LBFGS) | 实现 L-BFGS 算法，深受[minFunc](https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html)启发。 |\n",
    "| [`NAdam`](https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html#torch.optim.NAdam) | 实现 NAdam 算法。                                            |\n",
    "| [`RAdam`](https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html#torch.optim.RAdam) | 实现 RAdam 算法。                                            |\n",
    "| [`RMSprop`](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop) | 实现 RMSprop 算法。                                          |\n",
    "| [`Rprop`](https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop) | 实现弹性反向传播算法。                                       |\n",
    "| [`SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) | 实现随机梯度下降（可选用动量）。                             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80e5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
