{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "英语一般不需要分词。但存在拼写、单复数、时态等问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查英文拼写是否错误\n",
    "pip install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: peope\n",
      "ERROR: likee\n"
     ]
    }
   ],
   "source": [
    "from enchant.checker import SpellChecker\n",
    "chkr = SpellChecker(\"en_US\")\n",
    "chkr.set_text(\"Many peope likee to watch In the Name of People.\")\n",
    "for err in chkr:\n",
    "    print(\"ERROR:\", err.word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK安装\n",
    "pip install nltk  \n",
    "下载语料库nltk_data到anaconda的lib下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer # import Porter stemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "pst = PorterStemmer()    # create obj of the PorterStemmer\n",
    "lst = LancasterStemmer() # create obj of LancasterStemmer\n",
    "lst.stem(\"eating\")\n",
    "pst.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "print(\" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'countri'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\") # Choose a language\n",
    "stemmer.stem(\"countries\") # Stem a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('countries'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转换大小写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引入停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 15)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 16)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 5)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 13)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 10)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "vectorizer=CountVectorizer()\n",
    "corpus=[\"I come to China to travel\", \n",
    "    \"This is a car polupar in China\",          \n",
    "    \"I love tea and Apple \",   \n",
    "    \"The work is to write some papers in science\"] \n",
    "print(vectorizer.fit_transform(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]\n",
      " [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]\n",
      "['and', 'apple', 'car', 'china', 'come', 'in', 'is', 'love', 'papers', 'polupar', 'science', 'some', 'tea', 'the', 'this', 'to', 'travel', 'work', 'write']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.fit_transform(corpus).toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t2.0\n",
      "  (0, 2)\t-1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t-1.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (1, 2)\t-1.0\n",
      "  (1, 5)\t-1.0\n",
      "  (2, 0)\t2.0\n",
      "  (2, 5)\t-2.0\n",
      "  (3, 0)\t0.0\n",
      "  (3, 1)\t4.0\n",
      "  (3, 2)\t-1.0\n",
      "  (3, 3)\t1.0\n",
      "  (3, 5)\t-1.0\n"
     ]
    }
   ],
   "source": [
    "#Hash Trick\n",
    "from sklearn.feature_extraction.text import HashingVectorizer \n",
    "vectorizer2=HashingVectorizer(n_features = 6,norm = None)\n",
    "print(vectorizer2.fit_transform(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16)\t0.4424621378947393\n",
      "  (0, 3)\t0.348842231691988\n",
      "  (0, 15)\t0.697684463383976\n",
      "  (0, 4)\t0.4424621378947393\n",
      "  (1, 5)\t0.3574550433419527\n",
      "  (1, 9)\t0.45338639737285463\n",
      "  (1, 2)\t0.45338639737285463\n",
      "  (1, 6)\t0.3574550433419527\n",
      "  (1, 14)\t0.45338639737285463\n",
      "  (1, 3)\t0.3574550433419527\n",
      "  (2, 1)\t0.5\n",
      "  (2, 0)\t0.5\n",
      "  (2, 12)\t0.5\n",
      "  (2, 7)\t0.5\n",
      "  (3, 10)\t0.3565798233381452\n",
      "  (3, 8)\t0.3565798233381452\n",
      "  (3, 11)\t0.3565798233381452\n",
      "  (3, 18)\t0.3565798233381452\n",
      "  (3, 17)\t0.3565798233381452\n",
      "  (3, 13)\t0.3565798233381452\n",
      "  (3, 5)\t0.2811316284405006\n",
      "  (3, 6)\t0.2811316284405006\n",
      "  (3, 15)\t0.2811316284405006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf2 = TfidfVectorizer()\n",
    "re = tfidf2.fit_transform(corpus)\n",
    "print(re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立分析模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 在**nltk.corpus**包下，提供了几类标注好的语料库\n",
    "\n",
    "| **语料库**    | **说明**                                                     |\n",
    "| ------------- | ------------------------------------------------------------ |\n",
    "| **gutenberg** | **一个有若干万部的小说语料库，多是古典作品**                 |\n",
    "| **webtext**   | **收集的网络广告等内容**                                     |\n",
    "| **nps_chat**  | **有上万条聊天消息语料库，即时聊天消息为主**                 |\n",
    "| **brown**     | **一个百万词级的英语语料库，按文体进行分类**                 |\n",
    "| **reuters**   | **路透社语料库，上万篇新闻方档，约有1百万字，分90个主题，并分为训练集和测试集两组** |\n",
    "| **inaugural** | **演讲语料库，几十个文本，都是总统演说**                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "57340\n",
      "1161192\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "print(brown.categories())   #输出brown语料库的类别\n",
    "print(len(brown.sents()))   #输出brown语料库的句子数量\n",
    "print(len(brown.words()))   #输出brown语料库的词数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Welcome readers.', 'I hope you find it interesting.', 'Please doreply.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text=\" Welcome readers. I hope you find it interesting. Please doreply.\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Hello everyone.',\n",
       " 'Hope all are fine and doing well.',\n",
       " 'Hope you find the book interesting']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "french_tokenizer=nltk.data.load('tokenizers/punkt/french.pickle') # 法语、也有其他语言\n",
    "text=\" Hello everyone. Hope all are fine and doing well. Hope you find the book interesting\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Everyone', '!', 'hola', 'gr8']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Hi Everyone ! hola gr8\"\n",
    "s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Everyone', '!', 'hola', 'gr8']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Everyone', 'hola', 'gr8']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
    "regexp_tokenize(s, pattern='\\w+')\n",
    "#regexp_tokenize(s, pattern='\\d+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 停用词移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', '!']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "text = \"This is just a test !\"\n",
    "cleanwordlist = [word for word in text.lower().split() if word not in stoplist]\n",
    "cleanwordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 罕见词移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a', ' ', 'b'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist = nltk.FreqDist('a a a a a a a a a b')\n",
    "rarewords = freq_dist.keys()\n",
    "rarewords\n",
    "#after_rare_words = [ word for word in token not in rarewords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词性标注"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NNP 专用名词的单数形式\n",
    "NNPS 专用名词的复数形式\n",
    "PDT 前置限定词\n",
    "POS 所有格结束符\n",
    "PRP 人称代词\n",
    "PRP$ 所有格代词\n",
    "RB 副词\n",
    "RBR 相对副词\n",
    "RBS 最高级副词\n",
    "RP 小品词\n",
    "SYM 符号（数学符号或特殊符号）\n",
    "TO To\n",
    "UH 叹词\n",
    "VB 动词的基本形式\n",
    "VBD 动词的过去式\n",
    "VBG 动词的动名词用法\n",
    "VBN 动词的过去分词\n",
    "WP Wh-代词\n",
    "WP$ 所有格wh-代词\n",
    "WRB Wh-副词\n",
    "# 井号符\n",
    "$ 美元符\n",
    ". 句号\n",
    ", 逗号\n",
    ": 分号，分隔符\n",
    "( 左括号\n",
    ") 右括号\n",
    "\" 直双引号\n",
    "' 左单引号\n",
    "\" 左双引号\n",
    "' 右单引号\n",
    "\" 右双引号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('was', 'VBD'), ('watching', 'VBG'), ('TV', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "s = \"I was watching TV\"\n",
    "print(nltk.pos_tag(word_tokenize(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
